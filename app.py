import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

"""
@st.cache_resource
def load_model():
    model_name = "D:\stage PFE 2025\les models √† garder bien entrainer\qwen2.5 0.5B\saved_model"
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32,  # Utiliser float32 sur CPU
        
    ).to('cpu')

    # V√©rifier et d√©sactiver l'attention par fen√™tre glissante si n√©cessaire
    if hasattr(model.config, 'use_sliding_window_attention'):
        model.config.use_sliding_window_attention = False

    # V√©rifier le type d'attention
    if hasattr(model.config, 'attention_type'):
        model.config.attention_type = "softmax"
    return tokenizer, model

tokenizer, model = load_model()

st.title("üöÄ G√©n√©rateur de texte avec mon mod√®le")

prompt = st.text_area("Entrer le texte initial :", value="")

max_length = st.slider("Longueur maximale du texte g√©n√©r√© :", min_value=10, max_value=200, value=50)
temperature = st.slider("Temp√©rature (cr√©ativit√©) :", min_value=0.1, max_value=1.0, value=0.7, step=0.1)

if st.button("G√©n√©rer le texte"):
    if prompt.strip() == "":
        st.warning("Veuillez entrer un texte initial.")
    else:
        with st.spinner("G√©n√©ration en cours..."):
            input_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)
            output_ids = model.generate(
                input_ids,
                max_length=max_length,
                temperature=temperature,
                do_sample=True,
                top_k=50,
                top_p=0.95,
                pad_token_id=tokenizer.eos_token_id
            )
            output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
            st.success("üéâ Texte g√©n√©r√© :")
            st.write(output)
"""

@st.cache_resource
def load_model():
    model_name = "D:\stage PFE 2025\les models √† garder bien entrainer\qwen2.5 0.5B\saved_model"
    
    # Charger le tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Charger le mod√®le en for√ßant l'utilisation du CPU
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32
    ).to('cpu')

    # V√©rifier et d√©sactiver l'attention par fen√™tre glissante si n√©cessaire
    if hasattr(model.config, 'use_sliding_window_attention'):
        model.config.use_sliding_window_attention = False

    # V√©rifier le type d'attention et d√©finir "softmax" si applicable
    if hasattr(model.config, 'attention_type'):
        model.config.attention_type = "softmax"
    
    return tokenizer, model

# Charger le mod√®le et le tokenizer
tokenizer, model = load_model()

# Interface Streamlit
st.title("üöÄ G√©n√©rateur de texte avec mon mod√®le Qwen2.5-0.5B")

# Zone de texte pour le prompt
prompt = st.text_area("Entrer le texte initial :", value="")

# Param√®tres de g√©n√©ration
max_length = st.slider("Longueur maximale du texte g√©n√©r√© :", min_value=10, max_value=200, value=50)
temperature = st.slider("Temp√©rature (cr√©ativit√©) :", min_value=0.1, max_value=1.0, value=0.7, step=0.1)

if st.button("G√©n√©rer le texte"):
    if prompt.strip() == "":
        st.warning("Veuillez entrer un texte initial.")
    else:
        with st.spinner("G√©n√©ration en cours..."):
            try:
                # Encoder le prompt
                input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to('cpu')
                
                # G√©n√©ration du texte
                output_ids = model.generate(
                    input_ids,
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True,
                    top_k=50,
                    top_p=0.95,
                    pad_token_id=tokenizer.eos_token_id
                )
                
                # D√©coder et afficher le r√©sultat
                output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
                st.success("üéâ Texte g√©n√©r√© :")
                st.write(output)

            except Exception as e:
                st.error(f"Une erreur est survenue lors de la g√©n√©ration : {e}")
